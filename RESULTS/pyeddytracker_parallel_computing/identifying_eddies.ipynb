{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60837cea-2ea1-4256-9bdf-cd5b07faefbf",
   "metadata": {},
   "source": [
    "# Eddy identification based on 0.25deg grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc6f36-26b9-428a-a9da-b69e85291a52",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define paths to interpolated SSH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ce420-a3e7-45eb-b5ce-d291bbbddd10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from py_eddy_tracker.dataset.grid import RegularGridDataset\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "#Read in example SSH data that has been mapped onto a 0.25deg regular grid.\n",
    "expid='erc1011'\n",
    "varname='ssh'\n",
    "fq='dm'\n",
    "\n",
    "# path to access ssh data to identify eddies\n",
    "datadir = '/work/bm1344/k203123/reg25/erc1011/ssh/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f11cb5a-e37a-49e6-abd0-fa45cdcbfcd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "#find datafiles\n",
    "datafiles = sorted(glob.glob(datadir+\"*.nc\"))\n",
    "print('# data files for identifying eddies: ', len(datafiles))\n",
    "print('datafiles for identifying eddies: ', datafiles)\n",
    "#create datetime objects for 2002 to 2008; each year one entry in list\n",
    "datearrs = []\n",
    "for x in range(len(datafiles)):\n",
    "    yyyy=int(2002+x)\n",
    "    datearrs.append(np.arange(datetime(yyyy,1,1), datetime(yyyy+1,1,1), timedelta(days=1)).astype(datetime))\n",
    "print('datearrs: ', datearrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6ac1f-d65b-446d-9aa9-533a2c1966b3",
   "metadata": {},
   "source": [
    "### Start SLURMCluster on Levante for parallel computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef86678-52ff-43df-bf33-e796848d3bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b12a0-2bc5-4248-84d6-2de13b5d6a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# connect to dashboard to follow progress of computations\n",
    "dask.config.config.get('distributed').get('dashboard').update({'link':'{JUPYTERHUB_SERVICE_PREFIX}/proxy/{port}/status'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f744ac7-a5ba-4a0d-9126-a32a278292c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define cluster; corresponds to one SLURM job\n",
    "cluster = SLURMCluster(name='dask-cluster',\n",
    "                      cores=10,  \n",
    "                      memory='256GB',\n",
    "                      processes=5,\n",
    "                      interface='ib0',\n",
    "                      queue='compute',\n",
    "                      account='mh0033',    # account\n",
    "                      walltime='01:00:00', # job length\n",
    "                      asynchronous=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21c4cc-5ef8-413e-af0d-02c2792b4474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# multiply cluster size by 20; equivalent to 20 SLURM jobs\n",
    "cluster.scale(cores=200)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a46f4-51d6-4a27-9fa9-6fdaaf52cf69",
   "metadata": {},
   "source": [
    "### Define functions that we want to do in parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2eaead-8196-4bbd-bd37-a63ba8032d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function that identifies the eddies\n",
    "def detection(varfile,varname,date,tt,wavelength,shape_error):\n",
    "    # wavelength: choice of spatial cutoff for high pass filter in km\n",
    "    step_ht=0.005 #intervals to search for closed contours (5mm in this case)\n",
    "    g = RegularGridDataset(varfile, \"lon\", \"lat\", centered=True, indexs = dict(time=tt))\n",
    "    # date = datearr[tt] # detect each timestep individually because of memory issues\n",
    "    g.add_uv(varname)\n",
    "    g.bessel_high_filter(varname, wavelength, order=1)\n",
    "\n",
    "    a, c = g.eddy_identification(varname, \"u\", \"v\", \n",
    "    date,  # Date of identification\n",
    "    step_ht,  # step between two isolines of detection (m)\n",
    "    pixel_limit=(50, 400),  # Min and max pixel count for valid contour\n",
    "    shape_error=shape_error  # Error max (%) between ratio of circle fit and contour\n",
    "    )\n",
    "    return a,c,g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d2ad7-b16a-4196-b379-3d2573bf94ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "# function that writes data to netCDF files\n",
    "def detection_save_netcdf_output(varfile, varname, datearr, tt, wavelength, shape_error,fq):\n",
    "    outdir='/path/to/store/data/'+expid+'_eddytrack/wv_'+str(int(wavelength))+'_se_'+str(int(shape_error))+'/'\n",
    "    date = datearr[tt]\n",
    "    print('date = ', date)\n",
    "    print('tt = ', tt)\n",
    "    print('Identifying daily eddies for '+date.strftime('%Y%m%d'))\n",
    "    a_filtered, c_filtered, g_filtered = detection(varfile,varname,date,tt,wavelength,shape_error)\n",
    "    with Dataset(date.strftime(outdir+expid+\"_anticyclonic_\"+fq+\"_\"+date.strftime('%Y%m%d')+\".nc\"), \"w\") as h:\n",
    "        a_filtered.to_netcdf(h)\n",
    "    with Dataset(date.strftime(outdir+expid+\"_cyclonic_\"+fq+\"_\"+date.strftime('%Y%m%d')+\".nc\"), \"w\") as h:\n",
    "        c_filtered.to_netcdf(h)\n",
    "    del a_filtered\n",
    "    del c_filtered\n",
    "    del g_filtered\n",
    "    del date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a1173-0054-4d47-9d0b-b7290bb62932",
   "metadata": {},
   "source": [
    "### Use dask delayed to run eddy identification over specified number of timesteps in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3edcb4e-abe1-47a8-9c4b-0607ef2c84af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#looping over wavelengths for high band pass filter\n",
    "for wavelength in [200,700]:\n",
    "    #looping over shape_errors (%)\n",
    "    for shape_error in [30,70]:\n",
    "        print('wavelength, shape_error = ', wavelength, shape_error)\n",
    "        # looping over year (2002,2003...)\n",
    "        for i in range(len(datearrs)):\n",
    "            print('year = ', datearrs[i][0].year)\n",
    "            ntsteps_per_loop = 61\n",
    "            ntsteps = len(datearrs[i])\n",
    "            tcounter = np.zeros((ntsteps//ntsteps_per_loop)+2)\n",
    "            tcounter[:-1] = np.arange(0,(ntsteps//ntsteps_per_loop)+1)*ntsteps_per_loop\n",
    "            tcounter[-1] = ntsteps\n",
    "            # looping over each set of 61 time steps i.e. 0-60,61-121,122-182,183-243,244-304,305-365\n",
    "            for x in range(6):\n",
    "                print('tt vals = ', np.arange(tcounter[x],tcounter[x+1],1))\n",
    "                lazy_results = []\n",
    "                for tt in np.arange(tcounter[x],tcounter[x+1],1):\n",
    "                    # this defines the computation we want to do without actually doing it\n",
    "                    lazy_result = dask.delayed(detection_save_netcdf_output)(varfile=datafiles[i], \n",
    "                                                                             varname=varname, \n",
    "                                                                             datearr=datearrs[i], \n",
    "                                                                             tt=int(tt), \n",
    "                                                                             wavelength=wavelength, \n",
    "                                                                             shape_error=shape_error,\n",
    "                                                                             fq=fq)\n",
    "                    # save computations to be done in a list\n",
    "                    lazy_results.append(lazy_result)  \n",
    "                # do the computations for next 61 time steps stored in lazy_results\n",
    "                futures = dask.compute(*lazy_results)\n",
    "                results = dask.compute(*futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895756a-8c64-47f1-af2c-1c48fac5ae52",
   "metadata": {},
   "source": [
    "### Shutdown cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a67291b-f632-44c4-87ea-0b1c39661443",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7260d5-6b1e-452b-a0c0-b8fd1f3c592f",
   "metadata": {},
   "source": [
    "### Plotting one time step to check everything working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42ba5d-2bd5-405f-82e0-ff0e8e118e49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test plot\n",
    "\n",
    "tt=0\n",
    "i=0\n",
    "wavelength=700\n",
    "shape_error=30\n",
    "a0, c0, g0 = detection(datafiles[i],varname,datearrs[i][tt],tt,wavelength,shape_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f97304-0d39-42cc-97cc-6696eee8b533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start_axes(title):\n",
    "    fig = plt.figure(figsize=(13, 5))\n",
    "    ax = fig.add_axes([0.03, 0.03, 0.90, 0.94])\n",
    "    ax.set_xlim(0,360), ax.set_ylim(-75,75)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_title(title, weight=\"bold\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "def update_axes(ax, mappable=None):\n",
    "    ax.grid()\n",
    "    if mappable:\n",
    "        plt.colorbar(mappable, cax=ax.figure.add_axes([0.94, 0.05, 0.01, 0.9]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeca23f-fb0e-41c2-9843-e700e6e85040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = start_axes(\"Eddies detected over SSH for t=0\")\n",
    "m = g0.display(ax, \"ssh\", vmin=-0.15, vmax=0.15)\n",
    "a0.display(\n",
    "    ax,\n",
    "    lw=0.75,\n",
    "    label=\"Anticyclones in the filtered grid ({nb_obs} eddies)\",\n",
    "    ref=-10,\n",
    "    color=\"red\",\n",
    ")\n",
    "c0.display(\n",
    "    ax,\n",
    "    lw=0.75,\n",
    "    label=\"Cyclones in the filtered grid ({nb_obs} eddies)\",\n",
    "    ref=-10,\n",
    "    color=\"blue\",\n",
    ")\n",
    "ax.legend()\n",
    "update_axes(ax, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78310dbf-803f-4c23-a043-152cc53bcafb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytracker5",
   "language": "python",
   "name": "pytracker5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
